{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q1: Identify a specific question related to the humanities or the social sciences that you plan to address in your final project. You should outline why this is an interesting or important question and describe why computational methods are necessary and/or helpful in exploring this question. If possible, explain how others have answered/attempted to answer this question using different methods.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As decades pass, are newspaper headlines decreasing in complexity? In recent years, websites like The Huffington Post, Upworthy, and Buzzfeed have held greater and greater roles in America’s news sources, and their headlines tend to be simpler, flashier, and more engaging. Are other newspapers adopting these methods, or are they keeping constant with their old styles? If newspapers are slowly becoming more and more simple, this may signal a downward trend in general reading ability and attention span.\n",
    "\n",
    "Other scholars have done similar analysis on headlines, although other studies looked at relationships between how popular a headline gets and its word content or examined the most popular 3-word phrases. Computational methods are necessary for this problem, as we analyze thousands of headlines and calculating certain metrics like average length of word or number of unique words over a range of time. Collecting these headlines by hand is certainly infeasible and we would be here till next year if we attempted to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q2: Identify the corpus, or collection of texts, you will use to explore this question, and\n",
    "briefly describe why the identified corpus is appropriate. Additionally describe how you\n",
    "will collect the corpus and whether or not it will need to be cleaned prior to analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the corpora of New York Times article titles since the 1980s, so that we can examine if the increased usage of the internet and the World Wide Web for news has is correlated to any change in NYT headline complexity. This will be collected using a Python wrapper for the New York Times Article API. As the New York Times is one of the most well-known newspapers, the existence of a trend in the New York Times may be representative of a larger trend across mainstream media. Collecting the corpus will give us a wide range of extra data for each article; we simply need to select the headline from each article. One method of corpus cleaning that we are considering is to remove all proper nouns, though this may not be feasible as there is no obvious distinction between proper and common nouns in headlines since the first letter of each word is capitalized in every headline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q3: Describe the techniques you expect to use to analyze the text and explore your question.\n",
    "Why these techniques and not others? What kind of evidence will these techniques\n",
    "produce, and how will this help you answer the question and persuade others of your\n",
    "answer? Offer a hypothesis, or a few competing hypotheses, for what you think you may find. If you already have some preliminary analyses, include these as well. \n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a dictionary method to identify “click-bait”: specific words and phrases which produce significantly more interest in an article than others words and phrases. By counting the amount of click-bait, we can see if the usage of click-bait has significantly increased in recent years. We will also count the average number of words per headline and average number of letters per words; we can use certain statistical tests to determine if there is a statistically significant difference over time in our calculated metrics. Lastly, we also expect to calculate the number of unique words per month or per year. Using this, we may produce a graph that plots the number of unique words over a timespan against the number of articles in that time span. \n",
    "\n",
    "We believe that while the New York Times has resisted using click-bait, the headlines have gotten simpler over time. There is also the possibility that important events may strongly skew our data, in which case we will see outliers in our data in relevant time periods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q4: Briefly discuss any data visualization or interpretive techniques you will use to present\n",
    "your findings and convince others of your interpretation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make graphs of our data, mapping the year of the article against the value we are tracking. For example, we may make a graph showing average word count per article for each year, or we may make a graph showing total unique words in all the articles per year. We may also attempt using word clouds of words from specific decades to see if there exists a noticeable change over time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q5: If your project will be a team project, detail a planned division of labor and a shared\n",
    "contract about your collective expectations (I encourage you to revisit this document well\n",
    "before the final project is due, to check-in with each about about whether the shared\n",
    "contract is working). The best teams will include people from different disciplinary\n",
    "backgrounds so you can leverage each other's specialized knowledge, e.g. a computer\n",
    "scientist and a historian.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of us will be building our corpus of headlines based on code that Yitz wrote. Luis will be looking at academic sources to accompany theory about headlines and what they entail in journalism . After all of our corpus is combined, Yitz can give more insight into which types of statistical tests should be run, then both of us can perform analysis on our corpus using Pandas. Yitz can make the final graphs, while Luis writes up the analysis of the project. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
